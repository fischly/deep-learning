{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10\n",
    "### Kaggle link: [https://www.kaggle.com/c/cifar-10](https://www.kaggle.com/c/cifar-10)\n",
    "### WandB link: [https://wandb.ai/fischly/cifar10-with-resnet18](https://wandb.ai/fischly/cifar10-with-resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import everything needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as T\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING: It can take a lot of time to uncompress!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T18:02:46.954912Z",
     "iopub.status.busy": "2022-01-15T18:02:46.954538Z",
     "iopub.status.idle": "2022-01-15T18:03:34.61089Z",
     "shell.execute_reply": "2022-01-15T18:03:34.609396Z",
     "shell.execute_reply.started": "2022-01-15T18:02:46.954866Z"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m py7zr x /kaggle/input/cifar-10/train.7z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T18:03:34.612799Z",
     "iopub.status.busy": "2022-01-15T18:03:34.612518Z"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m py7zr x /kaggle/input/cifar-10/test.7z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = 'I:\\\\AI\\\\cifar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number training examples: 50000\n",
      "Number classes: 10\n"
     ]
    }
   ],
   "source": [
    "def read_csv_labels(fname):\n",
    "    \"\"\"Read `fname` to return a filename to label dictionary.\"\"\"\n",
    "    with open(fname, 'r') as f:\n",
    "        # Skip the file header line (column name)\n",
    "        lines = f.readlines()[1:]\n",
    "    tokens = [l.rstrip().split(',') for l in lines]\n",
    "    return dict(((name, label) for name, label in tokens))\n",
    "\n",
    "labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\n",
    "print(f'Number training examples: {len(labels)}')\n",
    "print(f'Number classes: {len(set(labels.values()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bird': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1], dtype=int8),\n",
       " 'dog': array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0], dtype=int8),\n",
       " 'horse': array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0], dtype=int8),\n",
       " 'truck': array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=int8),\n",
       " 'ship': array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0], dtype=int8),\n",
       " 'deer': array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0], dtype=int8),\n",
       " 'cat': array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0], dtype=int8),\n",
       " 'frog': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0], dtype=int8),\n",
       " 'automobile': array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int8),\n",
       " 'airplane': array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int8)}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a lookup-table with one-hot encoded values for our 10 classes\n",
    "classes = set(labels.values())\n",
    "ohe_table = { cl: np.array(list(bin(1 << i)[2:].zfill(len(classes))), dtype=np.int8) for i, cl in enumerate(list(classes)) }\n",
    "\n",
    "ohe_table_reverse = list(ohe_table.keys())\n",
    "def get_class_name_by_index(index):\n",
    "    classes = []\n",
    "    for idx in index:\n",
    "        classes.append(ohe_table_reverse[len(ohe_table) - 1 - idx.item()])\n",
    "    return classes\n",
    "\n",
    "ohe_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_train_valid(data_dir, labels, valid_ratio):\n",
    "    \"\"\"Split the validation set out of the original training set and returns file paths and labels for both the new sets.\"\"\"\n",
    "    # the number of examples of the class that has the fewest examples in the training dataset\n",
    "    n = collections.Counter(labels.values()).most_common()[-1][1]\n",
    "    \n",
    "    # the number of examples per class for the validation set\n",
    "    n_valid_per_label = max(1, math.floor(n * valid_ratio))\n",
    "    \n",
    "    # for storing the resulting file paths and labels\n",
    "    valid_file_paths = []\n",
    "    valid_labels = []\n",
    "    train_file_paths = []\n",
    "    train_labels = []\n",
    "    \n",
    "    label_count = {}\n",
    "    for train_file in os.listdir(os.path.join(data_dir, 'train')):\n",
    "        label = labels[train_file.split('.')[0]]\n",
    "        fname = os.path.join(data_dir, 'train', train_file)\n",
    "\n",
    "        if label not in label_count or label_count[label] < n_valid_per_label:\n",
    "            # mark as validation file\n",
    "            valid_file_paths.append(fname)\n",
    "            valid_labels.append(ohe_table[label])\n",
    "            \n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            # mark as train file\n",
    "            train_file_paths.append(fname)\n",
    "            train_labels.append(ohe_table[label])\n",
    "\n",
    "    return ((valid_file_paths, valid_labels), (train_file_paths, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for CIFAR images, where you can specify if you want to preload images into (V)RAM\n",
    "class CIFARDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None, train=False, test=False, preload=False):\n",
    "        self.transform = transform if train else T.Compose([T.ToTensor(), *transform.transforms])\n",
    "        \n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.preload = preload\n",
    "        \n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.number_of_files = len(file_paths)\n",
    "        \n",
    "        self.to_tensor = T.ToTensor()\n",
    "        \n",
    "        # to speed things up, we can preload the images into RAM/VRAM\n",
    "        if self.preload:\n",
    "            self.preloaded_images = []\n",
    "            \n",
    "            for file_path in tqdm(self.file_paths):\n",
    "                img = Image.open(file_path)\n",
    "\n",
    "                # if train, we only apply ToTensor transform. every other tranformation is done\n",
    "                # when __getitem__() is called. this is needed since we usually want to get a different transform\n",
    "                # for each item that is fetched from the dataset (due to RandomResizedCrop and RandomHorizontalFlip).\n",
    "\n",
    "                # if validation or test, we apply all the transformations right away.\n",
    "                # this is possible, since the transform is fixed for those cases, usually (e.g. Resize or CenterCrop).\n",
    "                transform_to_apply = self.to_tensor if self.train else self.transform\n",
    "\n",
    "                img_transformed = transform_to_apply(img).to(device)\n",
    "                self.preloaded_images.append(img_transformed)\n",
    "        \n",
    "        # create label tensor and move it to GPU\n",
    "        if not test:\n",
    "            self.labels = torch.tensor(np.array(self.labels), dtype=torch.float).to(device)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return self.number_of_files\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.preload:\n",
    "            if self.train:\n",
    "                return self.transform(self.preloaded_images[idx]), self.labels[idx]\n",
    "            else:\n",
    "                return self.preloaded_images[idx], self.labels[idx] if not self.test else os.path.basename(self.file_paths[idx]).split('.')[0]\n",
    "        else:\n",
    "            fp = self.file_paths[idx]\n",
    "            return self.transform(Image.open(fp)).to(device), self.labels[idx] if not self.test else os.path.basename(fp).split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "valid_ratio = 0.1\n",
    "\n",
    "# get the file paths for train and validation\n",
    "((valid_file_paths, valid_labels), (train_file_paths, train_labels)) = split_train_valid(data_dir, labels, valid_ratio)\n",
    "# get all files in the test directory as test files\n",
    "test_file_paths = glob.glob(os.path.join(data_dir, 'test', '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_train = torchvision.transforms.Compose([\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                [0.2023, 0.1994, 0.2010]),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomResizedCrop((32, 32), scale=(0.5, 1), ratio=(0.5, 2)),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.08)\n",
    "])\n",
    "\n",
    "transform_test = torchvision.transforms.Compose([\n",
    "    T.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (!) The next cell takes some time, because I load the train/validation images all to VRAM. This considerably speeds up learning later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFARDataset(train_file_paths, train_labels, transform_train, train=True, test=False, preload=True)\n",
    "valid_dataset = CIFARDataset(valid_file_paths, valid_labels, transform_test, train=False, test=False, preload=True)\n",
    "test_dataset  = CIFARDataset(test_file_paths, None, transform_test, train=False, test=True, preload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size, shuffle=True, drop_last=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, 2048, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_linear_layer(m, method):\n",
    "    torch.nn.init.xavier_normal_(m.weight, nn.init.calculate_gain(method))\n",
    "    torch.nn.init.constant_(m.bias, 0)\n",
    "    return m\n",
    "\n",
    "# load pretrained resnet18 model\n",
    "model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "# replace the last, dense layer with our own, that outputs only the 10 classes we have in the CIFAR10 dataset\n",
    "model.fc = init_linear_layer(nn.Linear(model.fc.in_features, 10), 'linear')\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr = 5e-4\n",
    "weight_decay = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfischly\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Binaries\\WPy64-31050\\notebooks\\pr-dl\\Ex6\\wandb\\run-20230113_145549-3w167txo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fischly/cifar10-with-resnet18/runs/3w167txo\" target=\"_blank\">avid-cloud-3</a></strong> to <a href=\"https://wandb.ai/fischly/cifar10-with-resnet18\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/fischly/cifar10-with-resnet18/runs/3w167txo?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1dd7cbb47c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"cifar10-with-resnet18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True, min_lr=1e-5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = 100000\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    accurate = 0\n",
    "    total = 0\n",
    "    losses = 0\n",
    "    \n",
    "    # --- TRAINING LOOP ---\n",
    "    model.train()\n",
    "    for X, y in tqdm(train_loader):\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        accurate += (torch.argmax(y, 1) == torch.argmax(y_pred, 1)).sum().float()\n",
    "        losses += loss.item()\n",
    "        total += len(y)\n",
    "\n",
    "        # zero the gradients before running the backward pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward pass to compute the gradient of loss w.r.t our learnable params\n",
    "        loss.backward()\n",
    "\n",
    "        # update params\n",
    "        optimizer.step()\n",
    "    \n",
    "    wandb.log({\n",
    "        'train/loss': losses / len(train_loader),\n",
    "        'train/accuracy': accurate / total\n",
    "    })\n",
    "    \n",
    "    # --- VALIDATION LOOP ---\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        correct = 0\n",
    "        for X, y in tqdm(valid_loader, leave=False):\n",
    "            y_pred = model(X)\n",
    "            val_loss += criterion(y_pred, y) * X.size(0)\n",
    "            \n",
    "            correct += (torch.argmax(y, 1) == torch.argmax(y_pred, 1)).sum().item()\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model, f'models/model_{epoch}.pt')\n",
    "        print(f'Saved model as models/model_{epoch}.pt')\n",
    "            \n",
    "    wandb.log({\n",
    "        'val/loss': val_loss / len(valid_dataset),\n",
    "        'val/accuracy': correct / len(valid_dataset)\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f'models/model_done_4.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23446a1947f4603b3eec8dc722a4ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = []\n",
    "with torch.inference_mode():\n",
    "    for X, lab in tqdm(test_loader):\n",
    "        y_pred = model(X)\n",
    "        #val_loss += criterion(y_pred, y) * X.size(0)\n",
    "        \n",
    "        predicted = torch.argmax(y_pred, 1).cpu()\n",
    "        predicted_class = get_class_name_by_index(predicted)\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            result.append({'id': lab[i], 'label': predicted_class[i]})\n",
    "            \n",
    "out_df = pd.DataFrame(result)\n",
    "out_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "# out_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
